{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQhiKziuqFyR"
   },
   "source": [
    "### Project Summary\n",
    "#####     In this project we have explored the data through the domain of unsupervised learning performing principal component analysis and clustering analysis. \n",
    "#####    One goal of this project is to best describe the variation in the different types of players. Doing so would equip us with insight into how to best choose players in a team. In a high-dimensional data, it is often difficult to develop an intuition of the features and our goal in this project is to reduce the dimensionality of the dataset so that we can visualize the relationships between the features and clusters in our dataset. \n",
    "#####    We start with 104 features and bring down the dimensionality to 60 features by selecting key features using our domain knowledge, and then into two principal components using PCA.  \n",
    "#####    visualize the clusters and evaluate the model using model evaluation metricds Silhouette_score \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgFJXfp4qFye"
   },
   "source": [
    "### Feature Selection\n",
    "\n",
    "Feature selection is the procedure to select the features (i.e. independent variables) automatically or manually those are more significant in terms of giving expected predictions option. Feature Selection is one amongst the core concepts in machine learning which massively affects the performance of the model. PCA method is used for feature selection of KMeans model.\n",
    "\n",
    "#### Features\n",
    "\n",
    "We have Removed the Unwanted And unrelated features like Id,name,dob etc. after that remaining related features are selected as below:-\n",
    "\n",
    "- age\t\n",
    "- height_cm\t\n",
    "- weight_kg\t\n",
    "- overall\t\n",
    "- potential\t\n",
    "- value_eur\t\n",
    "- wage_eur\t\n",
    "- international_reputation\t\n",
    "- weak_foot\t\n",
    "- skill_moves\n",
    "- team_jersey_number\n",
    "- real_face\n",
    "- contract_valid_until\n",
    "- nation_jersey_number\n",
    "- pace\n",
    "- shooting\n",
    "- passing\n",
    "- dribbling\n",
    "- defending\n",
    "- physic\n",
    "- gk_diving\t\n",
    "- gk_handling\t\n",
    "- gk_kicking\t\n",
    "- gk_reflexes\t\n",
    "- gk_speed\t\n",
    "- gk_positioning\t\n",
    "- player_traits\t\n",
    "- attacking_crossing\t\n",
    "- attacking_finishing\t\n",
    "- attacking_heading_accuracy\t\n",
    "- attacking_short_passing\t\n",
    "- attacking_volleys\t\n",
    "- skill_dribbling\t\n",
    "- skill_curve\t\n",
    "- skill_fk_accuracy\t\n",
    "- skill_long_passing \n",
    "- skill_ball_control\t\n",
    "- movement_acceleration\t\n",
    "- movement_sprint_speed\t\t\t\t\n",
    "- movement_agility\t\t \n",
    "- movement_reactions\n",
    "- movement_balance\n",
    "- power_shot_power\t \n",
    "- power_jumping\t\n",
    "- power_stamina\n",
    "- power_strength\n",
    "- power_long_shots\t\t\n",
    "- mentality_aggression\n",
    "- mentality_interceptions\n",
    "- mentality_positioning\t\n",
    "- mentality_vision\t\n",
    "- mentality_penalties\t\n",
    "- mentality_composure\t\n",
    "- defending_marking\t\n",
    "- defending_standing_tackle\t\n",
    "- defending_sliding_tackle\t\n",
    "- goalkeeping_diving\t\n",
    "- goalkeeping_handling\t\n",
    "- goalkeeping_kicking\t\n",
    "- goalkeeping_positioning\t\n",
    "- goalkeeping_reflexes\n",
    "\n",
    "#### Objective of this project:- Using clustering algorithms to categorize similar players using their traits and their different skills in the field.\n",
    "\n",
    "For clustering we have have to first prepare the data. \n",
    "\n",
    "###### Minmax Scaler is used for Data Pre-Processing\n",
    "\n",
    "#### Data Scaling\n",
    "\n",
    "Data Scaling is a data preprocessing step for numerical features. Many machine learning algorithms like Gradient descent methods, KNN algorithm, linear and logistic regression, etc. require data scaling to produce good results.Two types of scalaer are used: Standard Scaler and Minmax Scaler.\n",
    "Apart from supporting library functions other functions that will be used to achieve the functionality are:\n",
    "\n",
    "- The fit(data) method is used to compute the mean and std dev for a given feature so that it can be used further for scaling.\n",
    "- The transform(data) method is used to perform scaling using mean and std dev calculated using the .fit() method.\n",
    "- The fit_transform() method does both fit and transform.\n",
    "\n",
    "### Minmax Scaler\n",
    "\n",
    "This is the method of data scaling, where the minimum of feature is made equal to zero and the maximum of feature equal to one. MinMax Scaler shrinks the data within the given range, usually of 0 to 1. It transforms data by scaling features to a given range. It scales the values to a specific value range without changing the shape of the original distribution.\n",
    "\n",
    "The MinMax scaling is done using:\n",
    "\n",
    "x_std = (x – x.min(axis=0)) / (x.max(axis=0) – x.min(axis=0))\n",
    "\n",
    "x_scaled = x_std * (max – min) + min\n",
    "\n",
    "Where,\n",
    "\n",
    "- min, max = feature_range\n",
    "- x.min(axis=0) : Minimum feature value\n",
    "- x.max(axis=0):Maximum feature value\n",
    "\n",
    "Sklearn preprocessing defines MinMaxScaler() method to achieve this.\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\n",
    "\n",
    "In this project, we have decomposed the variables to 2 principal components. In order to obtain these, Sklearn decomposition defines PCA() method.\n",
    "\n",
    "#### Machine Learning Model\n",
    "\n",
    "#### The machine learning models used in this project is KMeans\n",
    "\n",
    "### KMeans\n",
    "\n",
    "K-Means Clustering is an Unsupervised Learning algorithm, which groups the unlabeled dataset into different clusters. Here K defines the number of pre-defined clusters that need to be created in the process, as if K=2, there will be two clusters, and for K=3, there will be three clusters, and so on.\n",
    "\n",
    "It allows us to cluster the data into different groups and a convenient way to discover the categories of groups in the unlabeled dataset on its own without the need for any training.\n",
    "\n",
    "It is a centroid-based algorithm, where each cluster is associated with a centroid. The main aim of this algorithm is to minimize the sum of distances between the data point and their corresponding clusters.\n",
    "\n",
    "The algorithm takes the unlabeled dataset as input, divides the dataset into k-number of clusters, and repeats the process until it does not find the best clusters. The value of k should be predetermined in this algorithm.\n",
    "\n",
    "The k-means clustering algorithm mainly performs two tasks:\n",
    "\n",
    "- Determines the best value for K center points or centroids by an iterative process.\n",
    "- Assigns each data point to its closest k-center. Those data points which are near to the particular k-center, create a cluster.\n",
    "\n",
    "Hence each cluster has datapoints with some commonalities, and it is away from other clusters.\n",
    "\n",
    "\n",
    "## The data achieved the accuracy rate with KMeans- 0.47733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLLocMc8qFyw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Fifa- Summary.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
